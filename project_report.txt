
SHL Assessment Recommendation System - Project Report

1. Approach Overview
--------------------
The goal was to build an intelligent recommendation system that maps natural language queries (job descriptions or role requirements) to specific SHL assessments. The solution is built using a Retrieval-Augmented Generation (RAG) pipeline with a hybrid search strategy.

1.1 Data Pipeline
- Ingestion: The SHL assessment catalog was scraped/ingested into a structured format (JSON).
- Indexing: 
    - Semantic Index: Used `sentence-transformers/all-mpnet-base-v2` to create dense vector embeddings of assessment descriptions and metadata. These are stored in a FAISS index for efficient semantic retrieval.
    - Keyword Index: Created a BM25 index using `rank_bm25` on tokenized assessment names and descriptions to capture exact keyword matches (crucial for specific tool names like "Excel" or "Java").

1.2 Retrieval Engine (Hybrid Search)
- Query Expansion: An LLM (Gemini 2.5 Flash via LangChain) is used to expand the user's query. It adds relevant synonyms and related skills (e.g., "Java" -> "Spring, Hibernate, OOP") to improve retrieval recall.
- Hybrid Retrieval: The system performs two parallel searches:
    1. Semantic Search (FAISS): Finds conceptually similar assessments.
    2. Keyword Search (BM25): Finds exact matches for tools and technologies.
- Fusion: Results are combined using Reciprocal Rank Fusion (RRF) to prioritize candidates that appear in both lists or rank highly in one.

1.3 Reranking & Selection
- The top candidates (e.g., 20) from the retrieval stage are passed to the LLM for reranking.
- The LLM is provided with the full metadata (name, duration, description) and a strict prompt to select the best 5-10 assessments.
- Logic: The prompt enforces client-specific priorities, such as preferring specific tool tests (e.g., "Microsoft Excel") over general role assessments when specific skills are mentioned.

2. Problems Faced & Solutions
-----------------------------

2.1 Problem: Low Recall on Specific Tools
- Issue: Initial semantic search often returned general "Software Engineer" assessments for queries asking specifically for "Java" or "Excel", missing the exact tool tests.
- Solution: Implemented Hybrid Search with BM25. BM25 is excellent at exact keyword matching, ensuring that if "Excel" is in the query, "Microsoft Excel" assessments are retrieved even if their semantic vector is slightly different from the query's intent.

2.2 Problem: LLM Output Formatting Errors
- Issue: The LLM used for reranking occasionally returned indices as strings (e.g., `["1", "2"]`) instead of integers, or included markdown formatting that broke the JSON parser. This caused the inference script to crash.
- Solution: Added robust error handling and type casting in `engine.py`. The code now sanitizes the LLM output (removing markdown backticks) and explicitly casts indices to integers before using them.

2.3 Problem: Balancing Hard and Soft Skills
- Issue: Queries often require a mix of technical skills (Hard Skills) and behavioral traits (Soft Skills). Simple retrieval might bias towards one.
- Solution: 
    1. Query Expansion: The LLM explicitly expands the query to include terms for both.
    2. Reranking Prompt: The system prompt explicitly instructs the LLM to "balance" the recommendations if the query implies multiple domains, ensuring a mix of "Knowledge & Skills" and "Personality & Behavior" tests.

2.4 Problem: "Vibe-Coding" vs. Reliability
- Issue: Relying solely on the LLM to "hallucinate" recommendations from memory is unreliable.
- Solution: The system is grounded in the retrieved context. The LLM *only* selects from the retrieved candidates and does not invent new assessments. This ensures all recommendations are valid URLs from the catalog.

3. Optimization Efforts
-----------------------
- Reciprocal Rank Fusion (RRF): Tuned the `k` parameter in RRF to balance the influence of keyword vs. semantic scores.
- Prompt Engineering: Iterated on the Reranking prompt to strictly enforce the "minimum 5, maximum 10" constraint and to prioritize "Individual Test Solutions" over "Pre-packaged Solutions" as per requirements.
- Performance: Used `faiss-cpu` for fast vector similarity search, ensuring the API responds within reasonable time limits.

4. Repository Structure for Submission
--------------------------------------
- `shl_recommender/`: Core package containing source code and data.
    - `src/engine.py`: Main logic for the recommendation engine.
    - `src/app.py`: FastAPI application serving the endpoints.
    - `data/`: Contains the FAISS index, metadata pickle, and raw JSON.
- `submission.csv`: The generated predictions for the test set.
- `requirements.txt`: List of dependencies.
- `project_report.txt`: This document.
- `README.md`: Instructions for running the code.

5. How to Run
-------------
1. Install dependencies: `pip install -r requirements.txt`
2. Set up `.env` with `GOOGLE_API_KEY`.
3. Run the API: `python shl_recommender/src/app.py`
4. Run inference: `python generate_submission.py`
